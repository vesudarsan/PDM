{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"Engine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1011001</th>\n",
       "      <th>1011002</th>\n",
       "      <th>1011003</th>\n",
       "      <th>1011004</th>\n",
       "      <th>1011005</th>\n",
       "      <th>1011006</th>\n",
       "      <th>1011007</th>\n",
       "      <th>1011008</th>\n",
       "      <th>1011009</th>\n",
       "      <th>1011010</th>\n",
       "      <th>...</th>\n",
       "      <th>1011264</th>\n",
       "      <th>1011265</th>\n",
       "      <th>1011266</th>\n",
       "      <th>1011267</th>\n",
       "      <th>1011270</th>\n",
       "      <th>1011271</th>\n",
       "      <th>1011272</th>\n",
       "      <th>1011273</th>\n",
       "      <th>1011466</th>\n",
       "      <th>1011467</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.0</td>\n",
       "      <td>3193.000000</td>\n",
       "      <td>3193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>267.043422</td>\n",
       "      <td>269.177611</td>\n",
       "      <td>267.043422</td>\n",
       "      <td>267.310197</td>\n",
       "      <td>267.576976</td>\n",
       "      <td>267.310197</td>\n",
       "      <td>267.310197</td>\n",
       "      <td>267.043422</td>\n",
       "      <td>349.067071</td>\n",
       "      <td>266.776643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.476924</td>\n",
       "      <td>53.423997</td>\n",
       "      <td>60.410993</td>\n",
       "      <td>512.149910</td>\n",
       "      <td>13330.702912</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.342769</td>\n",
       "      <td>34.136188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.324810</td>\n",
       "      <td>2.343388</td>\n",
       "      <td>2.324810</td>\n",
       "      <td>2.327131</td>\n",
       "      <td>2.329455</td>\n",
       "      <td>2.327131</td>\n",
       "      <td>2.327131</td>\n",
       "      <td>2.324810</td>\n",
       "      <td>3.705434</td>\n",
       "      <td>2.322490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.328585</td>\n",
       "      <td>0.228501</td>\n",
       "      <td>0.014453</td>\n",
       "      <td>0.130367</td>\n",
       "      <td>88.623238</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.632620</td>\n",
       "      <td>0.535294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>266.592102</td>\n",
       "      <td>268.722687</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>267.124756</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>348.369965</td>\n",
       "      <td>266.325775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.024872</td>\n",
       "      <td>53.089447</td>\n",
       "      <td>60.408234</td>\n",
       "      <td>512.126038</td>\n",
       "      <td>13314.745120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.227703</td>\n",
       "      <td>34.038826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>266.592102</td>\n",
       "      <td>268.722687</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>267.124756</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>348.369965</td>\n",
       "      <td>266.325775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.024872</td>\n",
       "      <td>53.408504</td>\n",
       "      <td>60.408302</td>\n",
       "      <td>512.126038</td>\n",
       "      <td>13314.765630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.227795</td>\n",
       "      <td>34.038902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>266.592102</td>\n",
       "      <td>268.722687</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>267.124756</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>348.369965</td>\n",
       "      <td>266.325775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.024872</td>\n",
       "      <td>53.409683</td>\n",
       "      <td>60.408318</td>\n",
       "      <td>512.126038</td>\n",
       "      <td>13314.776370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.227795</td>\n",
       "      <td>34.038902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>266.592102</td>\n",
       "      <td>268.722687</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>267.124756</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.858429</td>\n",
       "      <td>266.592102</td>\n",
       "      <td>348.369965</td>\n",
       "      <td>266.325775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.024872</td>\n",
       "      <td>53.409683</td>\n",
       "      <td>60.408333</td>\n",
       "      <td>512.126038</td>\n",
       "      <td>13314.776370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.227833</td>\n",
       "      <td>34.038937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>288.999207</td>\n",
       "      <td>291.308838</td>\n",
       "      <td>288.999207</td>\n",
       "      <td>289.287872</td>\n",
       "      <td>289.576599</td>\n",
       "      <td>289.287872</td>\n",
       "      <td>289.287872</td>\n",
       "      <td>288.999207</td>\n",
       "      <td>386.950500</td>\n",
       "      <td>288.710449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.468323</td>\n",
       "      <td>56.966232</td>\n",
       "      <td>60.549686</td>\n",
       "      <td>513.524719</td>\n",
       "      <td>14336.606450</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>60.0</td>\n",
       "      <td>47.330677</td>\n",
       "      <td>40.049034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1011001      1011002      1011003      1011004      1011005  \\\n",
       "count  3193.000000  3193.000000  3193.000000  3193.000000  3193.000000   \n",
       "mean    267.043422   269.177611   267.043422   267.310197   267.576976   \n",
       "std       2.324810     2.343388     2.324810     2.327131     2.329455   \n",
       "min     266.592102   268.722687   266.592102   266.858429   267.124756   \n",
       "25%     266.592102   268.722687   266.592102   266.858429   267.124756   \n",
       "50%     266.592102   268.722687   266.592102   266.858429   267.124756   \n",
       "75%     266.592102   268.722687   266.592102   266.858429   267.124756   \n",
       "max     288.999207   291.308838   288.999207   289.287872   289.576599   \n",
       "\n",
       "           1011006      1011007      1011008      1011009      1011010  ...  \\\n",
       "count  3193.000000  3193.000000  3193.000000  3193.000000  3193.000000  ...   \n",
       "mean    267.310197   267.310197   267.043422   349.067071   266.776643  ...   \n",
       "std       2.327131     2.327131     2.324810     3.705434     2.322490  ...   \n",
       "min     266.858429   266.858429   266.592102   348.369965   266.325775  ...   \n",
       "25%     266.858429   266.858429   266.592102   348.369965   266.325775  ...   \n",
       "50%     266.858429   266.858429   266.592102   348.369965   266.325775  ...   \n",
       "75%     266.858429   266.858429   266.592102   348.369965   266.325775  ...   \n",
       "max     289.287872   289.287872   288.999207   386.950500   288.710449  ...   \n",
       "\n",
       "       1011264      1011265      1011266      1011267      1011270  \\\n",
       "count   3193.0  3193.000000  3193.000000  3193.000000  3193.000000   \n",
       "mean       0.0   267.476924    53.423997    60.410993   512.149910   \n",
       "std        0.0     2.328585     0.228501     0.014453     0.130367   \n",
       "min        0.0   267.024872    53.089447    60.408234   512.126038   \n",
       "25%        0.0   267.024872    53.408504    60.408302   512.126038   \n",
       "50%        0.0   267.024872    53.409683    60.408318   512.126038   \n",
       "75%        0.0   267.024872    53.409683    60.408333   512.126038   \n",
       "max        0.0   289.468323    56.966232    60.549686   513.524719   \n",
       "\n",
       "            1011271      1011272  1011273      1011466      1011467  \n",
       "count   3193.000000  3193.000000   3193.0  3193.000000  3193.000000  \n",
       "mean   13330.702912     0.000140     60.0    40.342769    34.136188  \n",
       "std       88.623238     0.000317      0.0     0.632620     0.535294  \n",
       "min    13314.745120     0.000000     60.0    40.227703    34.038826  \n",
       "25%    13314.765630     0.000000     60.0    40.227795    34.038902  \n",
       "50%    13314.776370     0.000000     60.0    40.227795    34.038902  \n",
       "75%    13314.776370     0.000000     60.0    40.227833    34.038937  \n",
       "max    14336.606450     0.001156     60.0    47.330677    40.049034  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df.values,test_size =0.2, random_state =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2554, 66), (639, 66))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                1072      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 66)                1122      \n",
      "=================================================================\n",
      "Total params: 2,342\n",
      "Trainable params: 2,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 2664392.5000 - val_loss: 2421873.7500\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 1398834.6250 - val_loss: 228710.5000\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 27249.1641 - val_loss: 3.8519\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 7.293 - 0s 6ms/step - loss: 7.2372 - val_loss: 3.4121\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 2.6913 - val_loss: 1.7356\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.6727 - val_loss: 2.7384\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2.8827 - val_loss: 2.4889\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2.7878 - val_loss: 2.1712\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3.2297 - val_loss: 3.0690\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 3.2231 - val_loss: 1.9941\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2.8368 - val_loss: 2.7739\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2.8292 - val_loss: 1.5888\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.1128 - val_loss: 2.4994\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3.0243 - val_loss: 2.3815\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2.8310 - val_loss: 3.3942\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3.0656 - val_loss: 2.7280\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 2.8569 - val_loss: 1.7203\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.1152 - val_loss: 1.9405\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.8242 - val_loss: 2.8051\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.3054 - val_loss: 2.2760\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.4489 - val_loss: 3.6413\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.7424 - val_loss: 2.7118\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.9281 - val_loss: 4.0407\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2108 - val_loss: 2.5389\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.6121 - val_loss: 3.3303\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3.7313 - val_loss: 2.2245\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.1678 - val_loss: 2.6112\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3814 - val_loss: 5.2788\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.4155 - val_loss: 1.9430\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2.9835 - val_loss: 1.8520\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.0402 - val_loss: 3.3470\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.1290 - val_loss: 1.9705\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.1513 - val_loss: 5.8596\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.6800 - val_loss: 4.7144\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3450 - val_loss: 2.2608\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2954 - val_loss: 2.5796\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3923 - val_loss: 2.6497\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2595 - val_loss: 2.9314\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.4859 - val_loss: 3.5741\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3931 - val_loss: 2.1280\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.0729 - val_loss: 3.5265\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.0528 - val_loss: 2.3685\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.7704 - val_loss: 5.0428\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.5704 - val_loss: 1.8637\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.9672 - val_loss: 2.5434\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.4588 - val_loss: 3.8934\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.3192 - val_loss: 3.6746\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.7279 - val_loss: 2.4679\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.4316 - val_loss: 2.0953\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.8034 - val_loss: 2.7652\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5185 - val_loss: 5.4200\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.1876 - val_loss: 3.7894\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.2595 - val_loss: 2.8641\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.4292 - val_loss: 3.5269\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.6598 - val_loss: 4.8193\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.2185 - val_loss: 4.0312\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.1072 - val_loss: 2.5743\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.2801 - val_loss: 3.9953\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5377 - val_loss: 1.8598\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.8439 - val_loss: 2.4847\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.4796 - val_loss: 9.0254\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.4663 - val_loss: 4.6401\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.5676 - val_loss: 2.0395\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.3607 - val_loss: 5.2946\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.2314 - val_loss: 3.7439\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.9536 - val_loss: 2.8500\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3099 - val_loss: 5.1484\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.6292 - val_loss: 6.7208\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.7160 - val_loss: 3.5137\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.1044 - val_loss: 2.3211\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.1403 - val_loss: 3.8335\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.9193 - val_loss: 3.6298\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 3ms/step - loss: 5.2466 - val_loss: 2.5446\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.7163 - val_loss: 3.7228\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3622 - val_loss: 5.8204\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.7443 - val_loss: 5.3383\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3157 - val_loss: 4.1624\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.8131 - val_loss: 3.7846\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5714 - val_loss: 3.2990\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.5567 - val_loss: 5.0090\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.4282 - val_loss: 5.8275\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.5214 - val_loss: 3.9539\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4.1463 - val_loss: 2.7136\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.7090 - val_loss: 3.7746\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.9408 - val_loss: 2.1681\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5.7868 - val_loss: 3.6501\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.3356 - val_loss: 2.9661\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.5677 - val_loss: 3.3253\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.6939 - val_loss: 3.3381\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.2539 - val_loss: 3.0971\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.2958 - val_loss: 3.0623\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3.7837 - val_loss: 2.9467\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4.4066 - val_loss: 3.9563\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4.2382 - val_loss: 4.4345\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.6869 - val_loss: 9.1901\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.8105 - val_loss: 4.6363\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3.7510 - val_loss: 4.1202\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3.7451 - val_loss: 3.3040\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 4.3327 - val_loss: 3.2395\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 4.9294 - val_loss: 5.5559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f40c0623250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16,input_dim=train_data.shape[1],activation='relu'))\n",
    "model.add(Dense(4,activation = 'relu'))\n",
    "model.add(Dense(16,activation = 'relu'))\n",
    "model.add(Dense(train_data.shape[1]))# Multiple output neurons\n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "model.fit(train_data,train_data,verbose=1,epochs=100,batch_size=32, validation_split=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.447255579337788\n"
     ]
    }
   ],
   "source": [
    "resconstructions = model.predict(train_data)\n",
    "score1 = np.sqrt(metrics.mean_squared_error(resconstructions,train_data))\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.013916567274072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = tf.keras.losses.mae(resconstructions,train_data)\n",
    "np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3704800540527353"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.125356729432278"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = np.mean(train_loss) + 3*np.std(train_loss)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df.values,test_size =0.1, random_state =111)\n",
    "anomaly_data = train_data[0]\n",
    "\n",
    "# anomaly_data[[0]] = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.86668611], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "resconstructions_inf  = model.predict(anomaly_data.reshape(1,66))\n",
    "anomaly_loss_inf = tf.keras.losses.mae(resconstructions_inf,anomaly_data)\n",
    "print(anomaly_loss_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.128580880874031\n"
     ]
    }
   ],
   "source": [
    "score2 = np.sqrt(metrics.mean_squared_error(resconstructions_inf,anomaly_data.reshape(1,66)))\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float64, numpy=1.866686113742795>,\n",
       " 3.125356729432278)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_loss_inf[0],threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(True, shape=(), dtype=bool)\n",
      "Normal \n"
     ]
    }
   ],
   "source": [
    "inference_res = tf.math.less(anomaly_loss_inf[0],threshold)\n",
    "print(inference_res)\n",
    "if inference_res:\n",
    "    print(\"Normal \")\n",
    "else:\n",
    "    print(\"Anomaly Detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.86668611], shape=(1,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "resconstructions_inf1  = loaded_model.predict(anomaly_data.reshape(1,66))\n",
    "\n",
    "anomaly_loss_inf1 = tf.keras.losses.mae(resconstructions_inf1,anomaly_data)\n",
    "print(anomaly_loss_inf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = { \"threshold\":threshold}\n",
    "\n",
    "with open(\"threshold_value\", 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.125356729432278\n"
     ]
    }
   ],
   "source": [
    "with open(\"threshold_value\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(data['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
